}
extractUrls(x)
x <- 'http://www.r-project.org'
extractUrls <- function(x) {
library('stringi')
stopifnot(is.character(x),length(x) == 1)
con <- url(x,open='r')
lines <- readLines(con)
someregex <- '(https?):\\/\\/(www\\.)?[a-z0-9\\.:].*?(?=\\s)'
result <- stri_extract_all(lines,someregex)
result
}
extractUrls(x)
x <- 'http://www.r-project.org'
extractUrls <- function(x) {
library('stringi')
stopifnot(is.character(x),length(x) == 1)
con <- url(x,open='r')
lines <- readLines(con)
someregex <- '(https?):\\/\\/(www\\.)?[a-z0-9\\.:].*?(?=\\s)'
result <- stri_extract_all(str=lines,regex=someregex)
result
}
extractUrls(x)
x <- 'http://www.r-project.org'
extractUrls <- function(x) {
library('stringi')
stopifnot(is.character(x),length(x) == 1)
con <- url(x,open='r')
lines <- readLines(con)
someregex <- '(https?):\\/\\/(www\\.)?[a-z0-9\\.:].*?(?=\\s)'
result <- stri_extract_all(str=lines,regex=someregex)
unlist(result)
}
extractUrls(x)
x <- 'http://www.r-project.org'
extractUrls <- function(x) {
library('stringi')
stopifnot(is.character(x),length(x) == 1)
lines <- readLines(x)
someregex <- '(https?):\\/\\/(www\\.)?[a-z0-9\\.:].*?(?=\\s)'
result <- stri_extract_all(str=lines,regex=someregex)
unlist(result)
}
extractUrls(x)
x <- 'http://www.r-project.org'
extractUrls <- function(x) {
library('stringi')
stopifnot(is.character(x),length(x) == 1)
lines <- readLines(x)
someregex <- '(https?):\\/\\/(www\\.)?[a-z0-9\\.:].*?(?=\\s)'
result <- stri_extract_all_regex(str=lines,regex=someregex)
unlist(result)
result[!is.na(result)]
}
extractUrls(x)
x <- 'http://www.r-project.org'
extractUrls <- function(x) {
library('stringi')
stopifnot(is.character(x),length(x) == 1)
lines <- readLines(x)
someregex <- '(https?):\\/\\/(www\\.)?[a-z0-9\\.:].*?(?=\\s)'
result <- stri_extract_all_regex(str=lines,someregex)
unlist(result)
result[!is.na(result)]
}
extractUrls(x)
extractUrls <- function(x) {
library('stringi')
stopifnot(is.character(x),length(x) == 1)
lines <- readLines(x)
someregex <- '(https?):\\/\\/(www\\.)?[a-z0-9\\.:].*'
result <- stri_extract_all_regex(str=lines,someregex)
unlist(result)
result[!is.na(result)]
}
extractUrls(x)
library(data.table)
iris <- data.table(iris)
install.packages(data.table)
install.packages('data.table')
library(data.table)
iris <- data.table(iris)
typeof(irisdt)
mode(itisdt)
class(irisdt)
unclass(irisdt)
library(data.table)
irisdt <- data.table(iris)
typeof(irisdt)
mode(itisdt)
class(irisdt)
unclass(irisdt)
irisdt[c(1,10,150),]
irisdt[rep(c(TRUE,FALSE,FALSE,FALSE),len=150),]
getS3method("[","data.table")
irisdt$id <- past(irisdt$Species,rep(1:15,times=3))
irisdt$id <- paste(irisdt$Species,rep(1:15,times=3))
head(iris$id)
head(irisdt$id)
?sstkeyv
?setkeyv
irisdt$id1 <- paste(irisdt$Species,rep(1:15,times=3))
irisdt$id1 <- paste0(irisdt$Species,rep(1:15,times=3))
irisdt$id2a <- as.character(irisdt$Species)
irisdt$id2b <- rep(1:50,times3)
irisdt$id1 <- paste0(irisdt$Species,rep(1:15,times=3))
irisdt$id2a <- as.character(irisdt$Species)
irisdt$id2b <- rep(1:50,times=3)
irisdt <- irisdt[sample(1:nrom(irisdt)),]
irisdt <- irisdt[sample(1:nrow(irisdt)),]
irisdt <- data.table(iris, key="id1")
irisdt <- data.table(irisdt, key="id1")
head(irisdt)
example(data.table)
?J
x = seq(1,100,1)
y = range(1,100,1) + rnorm(99)
y = range(1,100,1) + rnorm(100)
y
y = seq(1,100,1) + rnorm(100)
y
y[50] = 100
y
plot(x,y)
x = seq(1,100,1)
y = seq(1,100,1) + 3*rnorm(100)
plot(x,y)
x = seq(1,100,1)
y = seq(1,100,1) + 3*rnorm(100)
y[50] = 100
plot(x,y)
cor(x,y)
x = seq(1,100,1)
y = seq(1,100,1) + 3*rnorm(100)
y[50] = 1000
plot(x,y)
cor(x,y)
cortest(x,y)
x = seq(1,100,1)
y = seq(1,100,1) + 3*rnorm(100)
y[50] = 500
plot(x,y)
cor(x,y)
cortest(x,y
library(car)
fit <- lm(mpg~disp+hp+wt+drat, data=mtcars)
library(car)
fit <- lm(mpg~disp+hp+wt+drat, data=mtcars)
fit
incluenceplot(x,y)
influenceplot(x,y)
influencePlot(x,y)
car
mpg
car.mpg
mtcars
influencePlot(mtcars$mpg,mtcars$wt)
influencePlot(fit)
library(car)
library(car)
fit <- lm(mpg~disp+hp+wt+drat, data=mtcars
influencePlot(fit)
library(e1071)
library(car)
install.package('car')
install.packages('car')
library(car)
fit <- lm(mpg~disp+hp+wt+drat, data=mtcars
influencePlot(fit)
library(car)
fit <- lm(mpg~disp+hp+wt+drat, data=mtcars
influencePlot(fit)
influencePlot(fit)
fit <- lm(mpg~disp+hp+wt+drat, data=mtcars
)
influencePlot(fit)
qqplot(fit)
shapiro.test(mpg)
shapiro.test(mtcars$mpg)
ncols(mtcars)
ncol(mtcars)
for col in 1:ncol(mtcars) {
shapiro.test(mtcars[,col])
}
for (col in 1:ncol(mtcars)) {
shapiro.test(mtcars[,col])
}
for (col in 1:ncol(mtcars)) {
shap <- shapiro.test(mtcars[,col])
print shap
}
for (col in 1:ncol(mtcars)) {
shap <- shapiro.test(mtcars[,col])
shap
}
shap
for (col in 1:ncol(mtcars)) {
shap[col] <- shapiro.test(mtcars[,col])
shap[col]
}
shap
qqPlot(fit, main='QQ Plot')
resids <- studres(fit)
library(MASS)
resids <- studres(fit)
hist(resids, freq = FALSE)
crplots(fit)
crPlots(fit)
install.package('AppliedPredictiveModeling')
install.packages('AppliedPredictiveModeling')
library(AppliedPrectiveModels)
data(solubility)
library(AppliedPredictiveModels)
data(solubility)
library(AppliedPredictiveModeling)
data(solubility)
solubility
solTrainXtrans
solTrainYtrans
solTrainY
fit <- lm(solTrainY,solTrainXtrans)
?lm
?lm.fit
fit <- lm.fit(solTrainY,solTrainXtrans)
fit <- lm.fit(solTrainXtrans,solTrainY)
fit <- lm.fit(solTrainXtrans,solTrainY)
fit <- lm.fit(solTrainXtrans,solTrainY)
solTrainXtrans <- as.matrix(solTrainXtrans)
solTrainY <- as.matrix(solTrainY)
fit <- lm.fit(solTrainXtrans,solTrainY)
fit
fit[1]
fit[2]
names(fit)
fit[effects]
fit$effects
cor(SolTrainXtrans)
cor(SolTrainXtrans)
solTrainXtrans <- as.data.frame(solTrainXtrans)
cor(SolTrainXtrans)
corr(SolTrainXtrans)
?cor
cor(SolTrainXtrans)
class(solTrainXtrans)
cor(SolTrainXtrans)
cor(SolTrainXtrans,solTrainY)
cor(SolTrainXtrans)
for (col in 1:ncol(solTrainXtrans)) {
shap[col] <- shapiro.test(solTrainXtrans[,col])
shap
}
shap
names(shap)
shap$pvalue
shap$p.value
for (col in 1:ncol(solTrainXtrans)) {
shap[,col] <- shapiro.test(solTrainXtrans[,col])
shap
}
shap <- data.frame()
for (col in 1:ncol(solTrainXtrans)) {
shap[col] <- shapiro.test(solTrainXtrans[,col])$p.test
shap
}
for (col in 1:ncol(solTrainXtrans)) {
shap[col,] <- shapiro.test(solTrainXtrans[,col])$p.test
shap
}
shap <- vector()
for (col in 1:ncol(solTrainXtrans)) {
shap[col] <- shapiro.test(solTrainXtrans[,col])$p.test
shap
}
shap
shapiro.test(solTrainXtrans[,col])$p.test
shapiro.test(solTrainXtrans[,2])$p.test
shapiro.test(solTrainXtrans[,2])
names(shapiro.test(solTrainXtrans[,2]))
shapiro.test(solTrainXtrans[,2])$p.value
shap <- vector()
for (col in 1:ncol(solTrainXtrans)) {
shap[col] <- shapiro.test(solTrainXtrans[,col])$p.value
shap
}
shap
sort(shap)
range(1:50)
range(1,50)
range(50)
range(3)
################################################################################
### R code from Applied Predictive Modeling (2013) by Kuhn and Johnson.
### Copyright 2013 Kuhn and Johnson
###
### R code to process the Kaggle grant application data.
###
### Required packages: plyr, caret, lubridate
################################################################################
## The plyr, caret and libridate packages are used in this script. The
## code can also be run using multiple cores using the ddply()
## function. See ?ddply to get more information.
##
## These computations will take a fair amount of time and may consume
## a non-trivial amount of memory in the process.
##
## Load required libraries
library(plyr)
library(caret)
library(lubridate)
## How many cores on the machine should be used for the data
## processing. Making cores > 1 will speed things up (depending on your
## machine) but will consume more memory.
cores <- 3
if(cores > 1){
library(doParallel)
library(foreach)
cl <- makeCluster(cores)
registerDoParallel(cl)
}
# Note: when using parallel=T, there are always a couple of warnings:
# Warning messages:
# 1: <anonymous>: ... may be used in an incorrect context: '.fun(piece, ...)'
# 2: <anonymous>: ... may be used in an incorrect context: '.fun(piece, ...)'
# The issue is open on github: https://github.com/hadley/plyr/issues/203
# It seems to be safe to ignore them
# there are many other parallel backends, (eg doParallel), it might be worth trying them
## Read in the data in it's raw form. Some of the column headings do
## not convert to proper R variable names, so many will contain dots,
## such as "Dept.No" instead of "Dept No"
setwd("PublicProjects/Grants/data")
raw <- read.csv("unimelb_training.csv")
## In many cases, missing values in categorical data will be converted
## to a value of "Unk"
raw$Sponsor.Code <- as.character(raw$Sponsor.Code)
raw$Sponsor.Code[raw$Sponsor.Code == ""] <- "Unk"
raw$Sponsor.Code <- factor(paste("Sponsor", raw$Sponsor.Code, sep = ""))
raw$Grant.Category.Code <- as.character(raw$Grant.Category.Code)
raw$Grant.Category.Code[raw$Grant.Category.Code == ""] <- "Unk"
raw$Grant.Category.Code <- factor(paste("GrantCat", raw$Grant.Category.Code, sep = ""))
raw$Contract.Value.Band...see.note.A <- as.character(raw$Contract.Value.Band...see.note.A)
raw$Contract.Value.Band...see.note.A[raw$Contract.Value.Band...see.note.A == ""] <- "Unk"
raw$Contract.Value.Band...see.note.A <- factor(paste("ContractValueBand", raw$Contract.Value.Band...see.note.A, sep = ""))
## Change missing Role.1 information to Unk
raw$Role.1 <- as.character(raw$Role.1)
raw$Role.1[raw$Role.1 == ""] <- "Unk"
## At this point, the data for investigators is in different
## columns. We'll take this "horizontal" format and convert it to a
## "vertical" format where the data are stacked. This will make some
## of the data processing easier.
## Split up the data by role number (1-15) and add any missing columns
## (roles 1-5 have more columns -20- than the others -16-)
# the difference is "RFCD.Code", "RFCD.Percentage", "SEO.Code", "SEO.Percentage"
# this loops produces a tmp list of tmpData data.frames, we then rbind them at the end
tmp <- vector(mode = "list", length = 15)
for(i in 1:15)  {
# get
tmpData <- raw[, c("Grant.Application.ID", grep(paste("\\.", i, "$", sep = ""), names(raw), value = TRUE))]
names(tmpData) <- gsub(paste("\\.", i, "$", sep = ""), "", names(tmpData))
if(i == 1) nms <- names(tmpData) # only the first one needs to have names, we'll rbind the rest
# add empty columns if they dont exist
if(all(names(tmpData) != "RFCD.Code")) tmpData$RFCD.Code <- NA
if(all(names(tmpData) != "RFCD.Percentage")) tmpData$RFCD.Percentage <- NA
if(all(names(tmpData) != "SEO.Code")) tmpData$SEO.Code <- NA
if(all(names(tmpData) != "SEO.Percentage")) tmpData$SEO.Percentage <- NA
tmp[[i]] <- tmpData[,nms]
rm(tmpData)
}
## Stack them up and remove any rows without role information
vertical <- do.call("rbind", tmp)
vertical <- subset(vertical, Role != "")
## Reformat variables:
# Role
# Year.of.Birth
# Country.of.Birth
# Home.Language
# Dept.No
# Faculty.NO
# RFCD.Code
# RFCD.Percent
## encode missing data or to make the factor levels more descriptive.
## to make complete factors, correctly
vertical$Role <- factor(as.character(vertical$Role))
## Get the unique values of the birth years and department codes.
bYears <- unique(do.call("c", raw[,grep("Year.of.Birth", names(raw), fixed = TRUE)]))
bYears <- bYears[!is.na(bYears)]
# year of birth, to factor
vertical$Year.of.Birth <- factor(paste(vertical$Year.of.Birth), levels = paste(sort(bYears)))
# fix country of birth, remove empty spaces
vertical$Country.of.Birth <- gsub(" ", "", as.character(vertical$Country.of.Birth))
vertical$Country.of.Birth[vertical$Country.of.Birth == ""] <- NA
vertical$Country.of.Birth <- factor(vertical$Country.of.Birth)
vertical$Home.Language <- gsub("Other", "OtherLang", as.character(vertical$Home.Language))
vertical$Home.Language[vertical$Home.Language == ""] <- NA
vertical$Home.Language <- factor(vertical$Home.Language)
vertical$Dept.No. <- paste("Dept", vertical$Dept.No., sep = "")
vertical$Dept.No.[vertical$Dept.No. == "DeptNA"] <- NA
vertical$Dept.No. <- factor(vertical$Dept.No.)
vertical$Faculty.No. <- paste("Faculty", vertical$Faculty.No., sep = "")
vertical$Faculty.No.[vertical$Faculty.No. == "FacultyNA"] <- NA
vertical$Faculty.No. <- factor(vertical$Faculty.No.)
vertical$RFCD.Code <- paste("RFCD", vertical$RFCD.Code, sep = "")
vertical$RFCD.Percentage[vertical$RFCD.Code == "RFCDNA"] <- NA
vertical$RFCD.Code[vertical$RFCD.Code == "RFCDNA"] <- NA
vertical$RFCD.Percentage[vertical$RFCD.Code == "RFCD0"] <- NA
vertical$RFCD.Code[vertical$RFCD.Code == "RFCD0"] <- NA
vertical$RFCD.Percentage[vertical$RFCD.Code == "RFCD999999"] <- NA
vertical$RFCD.Code[vertical$RFCD.Code == "RFCD999999"] <- NA
vertical$RFCD.Code <- factor(vertical$RFCD.Code)
vertical$SEO.Code <- paste("SEO", vertical$SEO.Code, sep = "")
vertical$SEO.Percentage[vertical$SEO.Code == "SEONA"] <- NA
vertical$SEO.Code[vertical$SEO.Code == "SEONA"] <- NA
vertical$SEO.Percentage[vertical$SEO.Code == "SEO0"] <- NA
vertical$SEO.Code[vertical$SEO.Code  == "SEO0"] <- NA
vertical$SEO.Percentage[vertical$SEO.Code == "SEO999999"] <- NA
vertical$SEO.Code[vertical$SEO.Code== "SEO999999"] <- NA
vertical$SEO.Code <- factor(vertical$SEO.Code)
vertical$No..of.Years.in.Uni.at.Time.of.Grant <- as.character(vertical$No..of.Years.in.Uni.at.Time.of.Grant)
vertical$No..of.Years.in.Uni.at.Time.of.Grant[vertical$No..of.Years.in.Uni.at.Time.of.Grant == ""] <- "DurationUnk"
vertical$No..of.Years.in.Uni.at.Time.of.Grant[vertical$No..of.Years.in.Uni.at.Time.of.Grant == ">=0 to 5"] <- "Duration0to5"
vertical$No..of.Years.in.Uni.at.Time.of.Grant[vertical$No..of.Years.in.Uni.at.Time.of.Grant == ">5 to 10"] <- "Duration5to10"
vertical$No..of.Years.in.Uni.at.Time.of.Grant[vertical$No..of.Years.in.Uni.at.Time.of.Grant == ">10 to 15"] <- "Duration10to15"
vertical$No..of.Years.in.Uni.at.Time.of.Grant[vertical$No..of.Years.in.Uni.at.Time.of.Grant == "more than 15"] <- "DurationGT15"
vertical$No..of.Years.in.Uni.at.Time.of.Grant[vertical$No..of.Years.in.Uni.at.Time.of.Grant == "Less than 0"] <- "DurationLT0"
vertical$No..of.Years.in.Uni.at.Time.of.Grant <- factor(vertical$No..of.Years.in.Uni.at.Time.of.Grant)
write.table(vertical, 'vertical.csv', sep='\t', row.names=F)
######################################################################
## A function to shorten the role titles
shortNames <- function(x, pre = "")
{
x <- gsub("EXT_CHIEF_INVESTIGATOR",  "ECI", x)
x <- gsub("STUD_CHIEF_INVESTIGATOR", "SCI", x)
x <- gsub("CHIEF_INVESTIGATOR",      "CI", x)
x <- gsub("DELEGATED_RESEARCHER",    "DR", x)
x <- gsub("EXTERNAL_ADVISOR",        "EA", x)
x <- gsub("HONVISIT",                "HV", x)
x <- gsub("PRINCIPAL_SUPERVISOR",    "PS", x)
x <- gsub("STUDRES",                 "SR", x)
x <- gsub("Unk",                     "UNK", x)
other <- x[x != "Grant.Application.ID"]
c("Grant.Application.ID", paste(pre, other, sep = ""))
}
## A function to find and remove zero-variance ("ZV") predictors
noZV <- function(x)
{
keepers <- unlist(lapply(x, function(x) length(unique(x)) > 1))
x[,keepers,drop = FALSE]
}
#========================================================
# Split
#========================================================
######################################################################
## Calculate the total number of people identified on the grant
library(plyr)
count(vertical$Grant.Application.ID)
people <- ddply(vertical, .(Grant.Application.ID), function(x) c(numpeople = nrow(x)))
######################################################################
## Calculate the number of people per role
# that is, a table with Grant.Application.ID NumCI NumDR NumECI NumEA NumHV NumPS NumSCI NumSR NumUNK ... etc
peoplePerRole <- ddply(vertical, .(Role), function(x) c(numpeople = nrow(x)))
######################################################################
## For each role, calculate the frequency of people in each age group
# that is, a table with Grant.Application.ID CI.1900 CI.1925 CI.1930 CI.1935 CI.1940 DR.1940 etc
AgesPerRole <- ddply(vertical, .(Role,Year.of.Birth), function(x) c(numpeople = nrow(x)))
######################################################################
## For each role, calculate the frequency of people from each country
# that is: names(investCountry)
# [1] "Grant.Application.ID"   "CI.AsiaPacific"         "DR.AsiaPacific"
# [4] "PS.AsiaPacific"         "CI.Australia"           "DR.Australia"
# [7] "ECI.Australia"          "HV.Australia"           "PS.Australia", etc
PeoplePerCountry <- ddply(vertical, .(Role,Country.of.Birth), function(x) c(numpeople = nrow(x)))
######################################################################
## For each role, calculate the frequency of people for each language
# that is: names(investLang)
# [1] "Grant.Application.ID" "CI.English"           "DR.English"
# [4] "ECI.English"          "PS.English"           "CI.OtherLang"
# [7] "DR.OtherLang"
LanguageforRoles <- ddply(vertical, .(Role,Home.Language), function(x) c(numpeople = nrow(x)))
######################################################################
## For each role, determine who as a Ph.D.
# that is: > names(investPhD)
# [1] "Grant.Application.ID" "CI.PhD"               "DR.PhD"
# [4] "ECI.PhD"              "HV.PhD"               "PS.PhD"
# [7] "SR.PhD"
PHDforRoles <- ddply(vertical, .(Role,With.PHD), function(x) c(numpeople = nrow(x)))
######################################################################
## For each role, calculate the number of successful and unsuccessful
## grants (**)
# that is:  names(investGrants)
# [1] "Grant.Application.ID" "Success.CI"           "Unsuccess.CI"
# [4] "Success.DR"           "Unsuccess.DR"         "Success.ECI"
# [7] "Unsuccess.ECI"        "Success.PS"           "Unsuccess.PS"
# [10] "Success.HV"           "Unsuccess.HV"         "Success.SR"
# [13] "Unsuccess.SR"
RoleSuccess <- ddply(vertical, .(Role), function(x) c(success = sum(x$Number.of.Successful.Grant),failure = sum(x$Number.of.Unsuccessful.Grant)))
RoleSucess
RoleSuccess
?ddply
dfx <- data.frame(
group = c(rep('A', 8), rep('B', 15), rep('C', 6)),
sex = sample(c("M", "F"), size = 29, replace = TRUE),
age = runif(n = 29, min = 18, max = 54)
)
dfx
ddply(dfx, .(group, sex), summarize,
mean = round(mean(age), 2),
sd = round(sd(age), 2))
?reshape
wide <- reshape(Indometh, v.names = "conc", idvar = "Subject",
timevar = "time", direction = "wide")
wide
library(reshape2)
